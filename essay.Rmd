---
output:
  bookdown::pdf_document2:
    highlight: tango
    toc: false
    number_sections: yes
    includes:
      in_header: preamble.tex
      before_body: title.tex
    citation_package: biblatex
bibliography: bibliography.bib
biblio-style: apa
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

# Performing an ANCOVA using R

## Installing and loading necessary packages

\vspace{0.5em}

In order to perform all the analysis in this tutorial, you will need to install the following packages:

- `datarium`: For accessing sample data;
- `tidyverse`: For data manipulation and plotting;
- `car`: To calculate the Type III analysis of variance (ANOVA) table and to execute the Levene's test;
- `emmeans`: To compute the estimated marginal means from the models.

Execute these commands to install the packages:

\vspace{0.5em}

```{r install-packs, eval=FALSE}
install.packages("tidyverse")
install.packages("car")
install.packages("emmeans")
```

\vspace{0.5em}

And then, these commands to load the packages into your R session.

\vspace{0.5em}

```{r load-packs, message=FALSE}
library(tidyverse)
library(car)
library(emmeans)
```

## Entering the data

For this tutorial, we will be using the `anxiety` dataset contained in the `datarium` package. As describe by the package manual, this data contains 45 individuals (`id` variable), divided into three groups (`group` variable), and their measured anxiety score at three time points (`t1`, `t2`, and `t3` variables). These data can be load and displayed with the following commands:

\vspace{0.5em}

```{r read-data}
data("anxiety", package = "datarium")
anxiety %>% print(n = Inf)
```

\vspace{0.5em}

In the subsequent analysis, we will only need the anxiety scores from the first and last time points. Therefore, we will select only these colums and change the varible names to `pre_test` and `post_test`, respectively. Also, we can rename the levels of the `group` variable to facilitate the interpretation: `grp1` will be renamed to `Control`, `grp2` to `Moderate`, and `grp3` to `High`, referring to the control group, and moderate and high intensity exercise groups, respectively. These changes can be done executing:

\vspace{0.5em}

```{r clean-data}
# Select and rename variables
anxiety <- anxiety %>% 
  as_tibble() %>% 
  select(id, group, pre_test = t1, post_test = t3)

# Recode exercise factors
anxiety$group <- recode_factor(
  anxiety$group,
  "grp1" = "Control",
  "grp2" = "Moderate",
  "grp3" = "High"
)
```

\vspace{0.5em}

And the data frame can, then, be inspected:

\vspace{0.5em}

```{r show-data}
anxiety
```

\vspace{0.5em}

The data, until now, is in wide format, with multiple observations of a same individual in the same line. Some analyses will need the data to be in the long format, with each observation of an individual in a different line. To create a new data frame in the long format, execute the following commands:

\vspace{0.5em}

```{r reshape-data}
# Reshape data
anxiety_long <- anxiety %>% 
  pivot_longer(
    c(pre_test, post_test),
    names_to = "time",
    values_to = "score"
  )

# Recode time into a factor
anxiety_long$time <- as_factor(anxiety_long$time)
anxiety_long$time <- recode_factor(
  anxiety_long$time, 
  "pre_test" = "Pre-test",
  "post_test" = "Post-test"
)

anxiety_long
```

\vspace{0.5em}

## Exploring the data

First, lets explore the data with some plots. We can generate boxplots for the three groups and two time points:

\vspace{0.5em}

```{r boxplot, fig.pos="H", out.extra="", out.width="75%", fig.align="center", fig.cap="Boxplot of anxiety score separated by group (Control, Moderate and High) and time (Pre-test and Post-test)."}
ggplot(data = anxiety_long, mapping = aes(x = group, y = score)) +
  geom_boxplot() +
  geom_dotplot(
    binaxis = "y",
    stackdir = "center",
    dotsize = 0.7,
    binwidth = 0.3,
    fill= "white"
  ) +
  facet_wrap(~time) +
  labs(x = "Group", y = "Anxiety score")
```

\vspace{0.5em}

As can be observed in the Figure \@ref(fig:boxplot) boxplots, pre-test anxiety score values are roughly equal among groups, while the post-test scores tend to be lower for the moderate, but mainly for the high intensty group, compared to the control. This indicates that there is at least a tendency to the exercise to decrease an individual`s anxiety level, and exercise intensity may play a role in it. Also, there is no clear difference in the spread of the scores among groups both time points.

Another useful plot to make is the histogram, as we need to verify whether the pre- and post-test scores are normally distributed in all of the groups [@Rutherford_2011], which is an assumption of parametric tests, such as the ANCOVA. We can plot a frequency histogram by executing the following code:

\vspace{0.5em}

```{r histogram-test, fig.pos="H", out.extra="", out.width="75%", fig.align="center", fig.cap="Histogram of the pre-test scores separated by group (Control, Moderate, High), without manual adjustment of the bin widths."}
ggplot(data = anxiety, mapping = aes(pre_test)) +
  geom_histogram() +
  facet_wrap(~group) +
  labs(x = "Pre-test", y = "")
```

\vspace{0.5em}

The resulting plot in Figure \@ref(fig:histogram-test) is not very illustrative of the data. A way to improve it would be to manually adjust the histogram bin width. The Freedman-Diaconis rule [@Freedman_1981] can be used to select the best width, and it is shown in Equation \@ref(eq:freedman).

\begin{equation}
  bin\ width = 2\frac{IQR(x)}{n^\frac{1}{3}}
  (\#eq:freedman)
\end{equation}

This rule can be written as a function in R, to be applied to the histograms.

\vspace{0.5em}

```{r bin-width}
bin_width <- function(variable) {
  bw <- 2 * IQR(variable) / length(variable)^(1/3)
  return(bw)
}
```

```{r histogram-pre, fig.pos="H", out.extra="", out.width="75%", fig.align="center", fig.cap="Histogram of the pre-test scores separated by group (Control, Moderate and High)."}
ggplot(data = anxiety, mapping = aes(pre_test)) +
  geom_histogram(binwidth = bin_width(anxiety$pre_test)) +
  facet_wrap(~group) +
  labs(x = "Pre-test", y = "")
```

```{r histogram-post, fig.pos="H", out.extra="", out.width="75%", fig.align="center", fig.cap="Histogram of the post-test scores separated by group (Control, Moderate and High)."}
ggplot(data = anxiety, mapping = aes(post_test)) +
  geom_histogram(binwidth = bin_width(anxiety$post_test)) +
  scale_x_continuous(breaks = seq(11, 21, 2)) +
  facet_wrap(~group) +
  labs(x = "Pre-test", y = "")
```

\vspace{0.5em}

By the inspection of the histograms in Figures \@ref(fig:histogram-pre) and \@ref(fig:histogram-post) we can notice that the data distribution is aproximately normal. To assure that, however, a normality test is needed. Prior to running the normality tests, we need to separate the original data frame into three, one for each group:

\vspace{0.5em}

```{r separate-groups}
# Separate the groups into 3 different data frames
anxiety_c <- filter(anxiety, group == "Control")
anxiety_m <- filter(anxiety, group == "Moderate")
anxiety_h <- filter(anxiety, group == "High")
```

\vspace{0.5em}

Now we can use the `shapiro.test()` function to perform the Shapiro-Wilk Normality Test in the pre-test scores of each of the groups:

\vspace{0.5em}

```{r normality-pre, results="hide"}
shapiro.test(anxiety_c$pre_test)
shapiro.test(anxiety_m$pre_test)
shapiro.test(anxiety_h$pre_test)
```

```{r normality-pre-results, echo=FALSE}
shapiro.test(anxiety_c$pre_test)
shapiro.test(anxiety_m$pre_test)
shapiro.test(anxiety_h$pre_test)
```

\vspace{0.5em}

And in the post-test scores:

\vspace{0.5em}

```{r normality-post, results="hide"}
shapiro.test(anxiety_c$post_test)
shapiro.test(anxiety_m$post_test)
shapiro.test(anxiety_h$post_test)
```

```{r normality-post-results, echo=FALSE}
shapiro.test(anxiety_c$post_test)
shapiro.test(anxiety_m$post_test)
shapiro.test(anxiety_h$post_test)
```

\vspace{0.5em}

As all *p* values were > 0.05, we can assume that both pre- and post-test scores of all groups had a normal distribution.

Then, another assumption of the parametric tests needs to be checked, which is the homogeneity of the variances [@Rutherford_2011; @Field_2012]. We can check if the variance of the outcome variable (post-test scores) is constant across groups by executing a Levene's test, which tests the null hypothesis that the variances in different groups are equal. In R, this test can be done by the `leveneTest()` function:

\vspace{0.5em}

```{r levene}
leveneTest(anxiety$post_test, anxiety$group, center = median)
```

\vspace{0.5em}

As the *p* value (located in the `Pr(>F)` column) is > 0.05, we assume that the null hypothesis is correct and that the variances are equal across the groups.

Finally, some descriptive statistics can be calculated, such as the sample size, mean and standard deviation of the scores:

\vspace{0.5em}

```{r descriptives}
descriptives <- anxiety_long %>% 
  group_by(group, time) %>% 
  summarise(
    n = n(),
    mean = mean(score),
    sd = sd(score)
  )
descriptives %>% as.data.frame()
```

\vspace{0.5em}

## Checking the assumptions

### Assumptions of parametric tests

The main assumptions of the parametric tests, which are the normality of data distribution and the homogeneity of variances have already been checked in the previous section.

### Independence of the covariate and treatment effect

To check this assumption, simply perform an ANOVA to test whether the experimental groups (control, medium and high) differ on the covariate (pre-test scores).

\vspace{0.5em}

```{r ind-cov-treat}
aov(formula = pre_test ~ group, data = anxiety) %>% summary()
```

\vspace{0.5em}

We can see that the *p* value (`Pr(>F)` column) is > 0.05. Therefore, we conclude that the covariate does not depend on the treatment effect.

### Linearity between the covariate and the outcome variable

In order to check this assumption, we need to build linear regression models using data from each group separately (`anxiety_c`, `anxiety_m` and `anxiety_h` data frames used previously). The models will have the post-test scores as outcome variable and the pre-test scores as predictor. This assumption will be met if the pre-test coefficients in the models are statistically significant (*p* < 0.05). Code to build the linear regression models and their results is shown below:

\vspace{0.5em}

```{r lm, results="hide"}
lm(formula = post_test ~ pre_test, data = anxiety_c) %>% summary()
lm(formula = post_test ~ pre_test, data = anxiety_m) %>% summary()
lm(formula = post_test ~ pre_test, data = anxiety_h) %>% summary()
```

```{r lm-results, echo=FALSE}
lm(formula = post_test ~ pre_test, data = anxiety_c) %>% summary()
lm(formula = post_test ~ pre_test, data = anxiety_m) %>% summary()
lm(formula = post_test ~ pre_test, data = anxiety_h) %>% summary()
```

\vspace{0.5em}

The coefficients *p* value can be found in the `Pr(>|t|)` column of the output. In this case, as the *p* values were very small, they are written in scientific notation. For example, on the first case (linear regression for the control group), the *p* value shown is `6.8e-08`, meaning $6.8 \times 10^{-08}$, which equals to 0.000000068. The results show that for each of the groups, there is a significant (*p* < 0.001) linear relationship between the covariate and the outcome variable. This relationship can be also shown in a scatterplot of the pre-test scores in the X axis and the post-test scores in the Y axis, with the linear regression lines for each group:

\vspace{0.5em}

```{r lm-plot, fig.pos="H", out.extra="", out.width="75%", fig.align="center", fig.cap="Scatterplot of pre-test X post-test scores, with linear regression lines. Groups (Control, Moderate and High) are distinguished by different point and line colors."}
ggplot(
  data = anxiety, 
  mapping = aes(x = pre_test, y = post_test, colour = group)
) +
  geom_point() +
  geom_smooth(
    method = "lm",
    se = FALSE
  ) +
  guides(color=guide_legend("group")) +
  labs(
    x = "Pre-test",
    y = "Post-test"
  )
```

\vspace{0.5em}

### Homegeneity of regression slopes

The first step to check this assumption would be to inspect the scatterplot with the regression lines in Figure \@ref(fig:lm-plot). If there is an homogeneity of regression slopes, then, the linear regression line of the three groups should be parallel. By looking at the plot, we can see that the lines are not exactly parallel, but have fairly similar slopes. An statistical test to confirm that is to build an ANOVA model with the post-test score as the outcome variable and the interaction term between the covariate and the treatment as the predictor. If this interaction efffect is not statistically significant, we can assume that the regression slopes are homogeneous.

\vspace{0.5em}

```{r hos}
aov(formula = post_test ~ pre_test * group, data = anxiety) %>% summary()
```

\vspace{0.5em}

In the output above, by looking at the line regarding the interaction term (`pre_test:group`), we can see that it is not statistically significant, as the *p* value (at the `Pr(> F)` column) is > 0.05.

\pagebreak